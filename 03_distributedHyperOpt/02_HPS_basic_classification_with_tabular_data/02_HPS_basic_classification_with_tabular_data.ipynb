{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4dc356b",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "<b>Reference</b>\n",
    "    \n",
    "This tutorial is based on materials from the Keras Documentation:\n",
    "* [Structured data classification from scratch](https://keras.io/examples/structured_data/structured_data_classification_from_scratch/)\n",
    "    \n",
    "</div>\n",
    "\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "<b>Warning</b>\n",
    "    \n",
    "By design asyncio does not allow nested event loops. Jupyter is using Tornado which already starts an event loop. Therefore the following patch is required to run this tutorial.\n",
    "    \n",
    "This tutorial should be run with `tensorFlow>=2.6`.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb4474f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nest_asyncio in /lus/grand/projects/datascience/regele/thetagpu/test/dhgpu/lib/python3.8/site-packages (1.5.1)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install nest_asyncio\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38664b59",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "<b>Note</b>\n",
    "    \n",
    "The following can be used to avoid the logging of Tensorflow *DEBUG*, *INFO* and *WARNING* statements.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "320e7d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = str(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0360c86e",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "    \n",
    "<b>Danger</b> \n",
    "\n",
    "The following cell contains Tensorflow import `import tensorflow as tf`. It is important to follow this strategy instead of `from tensorflow.keras.layers import ...` to avoid non-serializable data, creating crashed during the search. For example, the original Keras tutorial was using the following set of imports which was creating a serialization error in our use case.\n",
    "    \n",
    "```python\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "...\n",
    "from tensorflow.keras.layers import IntegerLookup\n",
    "from tensorflow.keras.layers import Normalization\n",
    "from tensorflow.keras.layers import StringLookup\n",
    "```\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3a37b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "import pandas as pd\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9061925",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "<b>Note</b>\n",
    "    \n",
    "The following can be used to detect if **GPU** devices are available on the current host.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da5e0602",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 GPU is available.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "def get_available_gpus():\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos if x.device_type == \"GPU\"]\n",
    "\n",
    "n_gpus = len(get_available_gpus())\n",
    "is_gpu_available = n_gpus > 0\n",
    "\n",
    "if is_gpu_available:\n",
    "    print(f\"{n_gpus} GPU{'s are' if n_gpus > 1 else ' is'} available.\")\n",
    "else:\n",
    "    print(\"No GPU available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c702c644",
   "metadata": {},
   "source": [
    "### The dataset (from Keras.io)\n",
    "\n",
    "The [dataset](https://archive.ics.uci.edu/ml/datasets/heart+Disease) is provided by the\n",
    "Cleveland Clinic Foundation for Heart Disease.\n",
    "It's a CSV file with 303 rows. Each row contains information about a patient (a\n",
    "**sample**), and each column describes an attribute of the patient (a **feature**). We\n",
    "use the features to predict whether a patient has a heart disease (**binary\n",
    "classification**).\n",
    "\n",
    "Here's the description of each feature:\n",
    "\n",
    "Column| Description| Feature Type\n",
    "------------|--------------------|----------------------\n",
    "Age | Age in years | Numerical\n",
    "Sex | (1 = male; 0 = female) | Categorical\n",
    "CP | Chest pain type (0, 1, 2, 3, 4) | Categorical\n",
    "Trestbpd | Resting blood pressure (in mm Hg on admission) | Numerical\n",
    "Chol | Serum cholesterol in mg/dl | Numerical\n",
    "FBS | fasting blood sugar in 120 mg/dl (1 = true; 0 = false) | Categorical\n",
    "RestECG | Resting electrocardiogram results (0, 1, 2) | Categorical\n",
    "Thalach | Maximum heart rate achieved | Numerical\n",
    "Exang | Exercise induced angina (1 = yes; 0 = no) | Categorical\n",
    "Oldpeak | ST depression induced by exercise relative to rest | Numerical\n",
    "Slope | Slope of the peak exercise ST segment | Numerical\n",
    "CA | Number of major vessels (0-3) colored by fluoroscopy | Both numerical & categorical\n",
    "Thal | 3 = normal; 6 = fixed defect; 7 = reversible defect | Categorical\n",
    "Target | Diagnosis of heart disease (1 = true; 0 = false) | Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9499cd94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    file_url = \"http://storage.googleapis.com/download.tensorflow.org/data/heart.csv\"\n",
    "    dataframe = pd.read_csv(file_url)\n",
    "\n",
    "    val_dataframe = dataframe.sample(frac=0.2, random_state=1337)\n",
    "    train_dataframe = dataframe.drop(val_dataframe.index)\n",
    "\n",
    "    return train_dataframe, val_dataframe\n",
    "\n",
    "\n",
    "def dataframe_to_dataset(dataframe):\n",
    "    dataframe = dataframe.copy()\n",
    "    labels = dataframe.pop(\"target\")\n",
    "    ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))\n",
    "    ds = ds.shuffle(buffer_size=len(dataframe))\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ceb5938",
   "metadata": {},
   "source": [
    "## Preprocessing & encoding of features\n",
    "\n",
    "The next cells use `tf.keras.layers.Normalization()` to apply standard scaling on the features. Then, the `tf.keras.layers.StringLookup` and `tf.keras.layers.IntegerLookup` are used to encode categorical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "996814d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_numerical_feature(feature, name, dataset):\n",
    "    # Create a Normalization layer for our feature\n",
    "    normalizer = tf.keras.layers.Normalization()\n",
    "\n",
    "    # Prepare a Dataset that only yields our feature\n",
    "    feature_ds = dataset.map(lambda x, y: x[name])\n",
    "    feature_ds = feature_ds.map(lambda x: tf.expand_dims(x, -1))\n",
    "\n",
    "    # Learn the statistics of the data\n",
    "    normalizer.adapt(feature_ds)\n",
    "\n",
    "    # Normalize the input feature\n",
    "    encoded_feature = normalizer(feature)\n",
    "    return encoded_feature\n",
    "\n",
    "\n",
    "def encode_categorical_feature(feature, name, dataset, is_string):\n",
    "    lookup_class = (\n",
    "        tf.keras.layers.StringLookup if is_string else tf.keras.layers.IntegerLookup\n",
    "    )\n",
    "    # Create a lookup layer which will turn strings into integer indices\n",
    "    lookup = lookup_class(output_mode=\"binary\")\n",
    "\n",
    "    # Prepare a Dataset that only yields our feature\n",
    "    feature_ds = dataset.map(lambda x, y: x[name])\n",
    "    feature_ds = feature_ds.map(lambda x: tf.expand_dims(x, -1))\n",
    "\n",
    "    # Learn the set of possible string values and assign them a fixed integer index\n",
    "    lookup.adapt(feature_ds)\n",
    "\n",
    "    # Turn the string input into integer indices\n",
    "    encoded_feature = lookup(feature)\n",
    "    return encoded_feature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8743319",
   "metadata": {},
   "source": [
    "## Define the run-function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f99f031",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(config: dict):\n",
    "    train_dataframe, val_dataframe = load_data()\n",
    "\n",
    "    train_ds = dataframe_to_dataset(train_dataframe)\n",
    "    val_ds = dataframe_to_dataset(val_dataframe)\n",
    "\n",
    "    train_ds = train_ds.batch(config[\"batch_size\"])\n",
    "    val_ds = val_ds.batch(config[\"batch_size\"])\n",
    "\n",
    "    # Categorical features encoded as integers\n",
    "    sex = tf.keras.Input(shape=(1,), name=\"sex\", dtype=\"int64\")\n",
    "    cp = tf.keras.Input(shape=(1,), name=\"cp\", dtype=\"int64\")\n",
    "    fbs = tf.keras.Input(shape=(1,), name=\"fbs\", dtype=\"int64\")\n",
    "    restecg = tf.keras.Input(shape=(1,), name=\"restecg\", dtype=\"int64\")\n",
    "    exang = tf.keras.Input(shape=(1,), name=\"exang\", dtype=\"int64\")\n",
    "    ca = tf.keras.Input(shape=(1,), name=\"ca\", dtype=\"int64\")\n",
    "\n",
    "    # Categorical feature encoded as string\n",
    "    thal = tf.keras.Input(shape=(1,), name=\"thal\", dtype=\"string\")\n",
    "\n",
    "    # Numerical features\n",
    "    age = tf.keras.Input(shape=(1,), name=\"age\")\n",
    "    trestbps = tf.keras.Input(shape=(1,), name=\"trestbps\")\n",
    "    chol = tf.keras.Input(shape=(1,), name=\"chol\")\n",
    "    thalach = tf.keras.Input(shape=(1,), name=\"thalach\")\n",
    "    oldpeak = tf.keras.Input(shape=(1,), name=\"oldpeak\")\n",
    "    slope = tf.keras.Input(shape=(1,), name=\"slope\")\n",
    "\n",
    "    all_inputs = [\n",
    "        sex,\n",
    "        cp,\n",
    "        fbs,\n",
    "        restecg,\n",
    "        exang,\n",
    "        ca,\n",
    "        thal,\n",
    "        age,\n",
    "        trestbps,\n",
    "        chol,\n",
    "        thalach,\n",
    "        oldpeak,\n",
    "        slope,\n",
    "    ]\n",
    "\n",
    "    # Integer categorical features\n",
    "    sex_encoded = encode_categorical_feature(sex, \"sex\", train_ds, False)\n",
    "    cp_encoded = encode_categorical_feature(cp, \"cp\", train_ds, False)\n",
    "    fbs_encoded = encode_categorical_feature(fbs, \"fbs\", train_ds, False)\n",
    "    restecg_encoded = encode_categorical_feature(restecg, \"restecg\", train_ds, False)\n",
    "    exang_encoded = encode_categorical_feature(exang, \"exang\", train_ds, False)\n",
    "    ca_encoded = encode_categorical_feature(ca, \"ca\", train_ds, False)\n",
    "\n",
    "    # String categorical features\n",
    "    thal_encoded = encode_categorical_feature(thal, \"thal\", train_ds, True)\n",
    "\n",
    "    # Numerical features\n",
    "    age_encoded = encode_numerical_feature(age, \"age\", train_ds)\n",
    "    trestbps_encoded = encode_numerical_feature(trestbps, \"trestbps\", train_ds)\n",
    "    chol_encoded = encode_numerical_feature(chol, \"chol\", train_ds)\n",
    "    thalach_encoded = encode_numerical_feature(thalach, \"thalach\", train_ds)\n",
    "    oldpeak_encoded = encode_numerical_feature(oldpeak, \"oldpeak\", train_ds)\n",
    "    slope_encoded = encode_numerical_feature(slope, \"slope\", train_ds)\n",
    "\n",
    "    all_features = tf.keras.layers.concatenate(\n",
    "        [\n",
    "            sex_encoded,\n",
    "            cp_encoded,\n",
    "            fbs_encoded,\n",
    "            restecg_encoded,\n",
    "            exang_encoded,\n",
    "            slope_encoded,\n",
    "            ca_encoded,\n",
    "            thal_encoded,\n",
    "            age_encoded,\n",
    "            trestbps_encoded,\n",
    "            chol_encoded,\n",
    "            thalach_encoded,\n",
    "            oldpeak_encoded,\n",
    "        ]\n",
    "    )\n",
    "    x = tf.keras.layers.Dense(config[\"units\"], activation=config[\"activation\"])(\n",
    "        all_features\n",
    "    )\n",
    "    x = tf.keras.layers.Dropout(config[\"dropout_rate\"])(x)\n",
    "    output = tf.keras.layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = tf.keras.Model(all_inputs, output)\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=config[\"learning_rate\"])\n",
    "    model.compile(optimizer, \"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "    history = model.fit(\n",
    "        train_ds, epochs=config[\"num_epochs\"], validation_data=val_ds, verbose=0\n",
    "    )\n",
    "\n",
    "    return history.history[\"val_accuracy\"][-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea800ab1",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    \n",
    "<b>Important</b>\n",
    "    \n",
    "The objective maximised by DeepHyper is the scalar value returned by the `run`-function. In this tutorial it corresponds to the validation accuracy of the last epoch of training which we retrieve in the `History` object returned by the `model.fit(...)` call.\n",
    "    \n",
    "```python\n",
    "...\n",
    "history = model.fit(\n",
    "    train_ds, epochs=config[\"num_epochs\"], validation_data=val_ds, verbose=0\n",
    ")\n",
    "\n",
    "return history.history[\"val_accuracy\"][-1]\n",
    "...\n",
    "```\n",
    "\n",
    "Using an objective like `max(history.history[\"val_accuracy\"])` can have undesired effect such as training curves passing by a local maximum and then dropping which will not generate a model in capacity of ingesting well more data in the future.\n",
    "    \n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1719e4",
   "metadata": {},
   "source": [
    "## Evaluate a default configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab7ab0c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "default_config = {\n",
    "    \"units\": 32,\n",
    "    \"activation\": \"relu\",\n",
    "    \"dropout_rate\": 0.5,\n",
    "    \"num_epochs\": 50,\n",
    "    \"batch_size\": 32,\n",
    "    \"learning_rate\": 1e-3,\n",
    "}\n",
    "\n",
    "if is_gpu_available:\n",
    "    \n",
    "    ray.init(num_cpus=n_gpus, num_gpus=n_gpus)\n",
    "    \n",
    "    run_default = ray.remote(num_cpus=1, num_gpus=1)(run)\n",
    "    \n",
    "    objective_default = ray.get(run_default.remote(default_config))\n",
    "else:\n",
    "    run_default = run\n",
    "    objective_default = run_default(default_config)\n",
    "    \n",
    "print(f\"Accuracy Default Configuration:  {objective_default:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca589b5",
   "metadata": {},
   "source": [
    "## Define the Hyperparameter optimization problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "82ba8e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTIVATIONS = [\n",
    "    \"elu\",\n",
    "    \"gelu\",\n",
    "    \"hard_sigmoid\",\n",
    "    \"linear\",\n",
    "    \"relu\",\n",
    "    \"selu\",\n",
    "    \"sigmoid\",\n",
    "    \"softplus\",\n",
    "    \"softsign\",\n",
    "    \"swish\",\n",
    "    \"tanh\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5813bb59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Configuration space object:\n",
       "  Hyperparameters:\n",
       "    activation, Type: Categorical, Choices: {elu, gelu, hard_sigmoid, linear, relu, selu, sigmoid, softplus, softsign, swish, tanh}, Default: elu\n",
       "    batch_size, Type: UniformInteger, Range: [8, 256], Default: 45, on log-scale\n",
       "    dropout_rate, Type: UniformFloat, Range: [0.0, 0.6], Default: 0.3\n",
       "    learning_rate, Type: UniformFloat, Range: [1e-05, 0.01], Default: 0.0003162278, on log-scale\n",
       "    num_epochs, Type: UniformInteger, Range: [10, 100], Default: 55\n",
       "    units, Type: UniformInteger, Range: [8, 128], Default: 68"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from deephyper.problem import HpProblem\n",
    "\n",
    "problem = HpProblem()\n",
    "problem.add_hyperparameter((8, 128), \"units\")\n",
    "problem.add_hyperparameter(ACTIVATIONS, \"activation\")\n",
    "problem.add_hyperparameter((0.0, 0.6), \"dropout_rate\")\n",
    "problem.add_hyperparameter((10, 100), \"num_epochs\")\n",
    "problem.add_hyperparameter((8, 256, \"log-uniform\"), \"batch_size\")\n",
    "problem.add_hyperparameter((1e-5, 1e-2, \"log-uniform\"), \"learning_rate\")\n",
    "\n",
    "problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596900f9",
   "metadata": {},
   "source": [
    "## Define the evaluator object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b3f0be13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method Arguments:  {'num_cpus': 1, 'num_cpus_per_task': 1, 'num_gpus': 1, 'num_gpus_per_task': 1}\n",
      "Evaluator's number of workers:  1\n"
     ]
    }
   ],
   "source": [
    "from deephyper.evaluator.evaluate import Evaluator\n",
    "\n",
    "# Default arguments for Ray: 1 worker and 1 worker per evaluation\n",
    "method_kwargs = {\n",
    "    \"num_cpus\": 1, \n",
    "    \"num_cpus_per_task\": 1\n",
    "}\n",
    "\n",
    "# If GPU devices are detected then it will create 'n_gpus' workers\n",
    "# and use 1 worker for each evaluation\n",
    "if is_gpu_available:\n",
    "    method_kwargs[\"num_cpus\"] = n_gpus\n",
    "    method_kwargs[\"num_gpus\"] = n_gpus\n",
    "    method_kwargs[\"num_cpus_per_task\"] = 1\n",
    "    method_kwargs[\"num_gpus_per_task\"] = 1\n",
    "    \n",
    "print(\"Method Arguments: \", method_kwargs)\n",
    "\n",
    "evaluator = Evaluator.create(\n",
    "    run, \n",
    "    method=\"ray\", \n",
    "    method_kwargs=method_kwargs\n",
    ")\n",
    "\n",
    "print(\"Evaluator's number of workers: \", evaluator.num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392998bc",
   "metadata": {},
   "source": [
    "## Define and run the asynchronous model-based search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1e00f0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deephyper.search.hps import AMBS\n",
    "\n",
    "search = AMBS(problem, evaluator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457546ce",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "results = search.search(max_evals=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "87bc93b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>activation</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>dropout_rate</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>num_epochs</th>\n",
       "      <th>units</th>\n",
       "      <th>id</th>\n",
       "      <th>objective</th>\n",
       "      <th>elapsed_sec</th>\n",
       "      <th>duration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>linear</td>\n",
       "      <td>17</td>\n",
       "      <td>0.311632</td>\n",
       "      <td>0.000676</td>\n",
       "      <td>71</td>\n",
       "      <td>97</td>\n",
       "      <td>1</td>\n",
       "      <td>0.786885</td>\n",
       "      <td>8.032742</td>\n",
       "      <td>7.282537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gelu</td>\n",
       "      <td>28</td>\n",
       "      <td>0.221418</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>60</td>\n",
       "      <td>68</td>\n",
       "      <td>2</td>\n",
       "      <td>0.721311</td>\n",
       "      <td>13.982644</td>\n",
       "      <td>5.657617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>linear</td>\n",
       "      <td>72</td>\n",
       "      <td>0.136193</td>\n",
       "      <td>0.000573</td>\n",
       "      <td>70</td>\n",
       "      <td>126</td>\n",
       "      <td>3</td>\n",
       "      <td>0.803279</td>\n",
       "      <td>18.642096</td>\n",
       "      <td>4.459822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gelu</td>\n",
       "      <td>98</td>\n",
       "      <td>0.394199</td>\n",
       "      <td>0.005590</td>\n",
       "      <td>65</td>\n",
       "      <td>118</td>\n",
       "      <td>4</td>\n",
       "      <td>0.786885</td>\n",
       "      <td>23.278052</td>\n",
       "      <td>4.436977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>selu</td>\n",
       "      <td>161</td>\n",
       "      <td>0.240164</td>\n",
       "      <td>0.000459</td>\n",
       "      <td>41</td>\n",
       "      <td>128</td>\n",
       "      <td>5</td>\n",
       "      <td>0.868852</td>\n",
       "      <td>26.683581</td>\n",
       "      <td>3.195485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>elu</td>\n",
       "      <td>168</td>\n",
       "      <td>0.396514</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>10</td>\n",
       "      <td>128</td>\n",
       "      <td>6</td>\n",
       "      <td>0.475410</td>\n",
       "      <td>29.111158</td>\n",
       "      <td>2.224259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>selu</td>\n",
       "      <td>34</td>\n",
       "      <td>0.064309</td>\n",
       "      <td>0.000546</td>\n",
       "      <td>13</td>\n",
       "      <td>114</td>\n",
       "      <td>7</td>\n",
       "      <td>0.836066</td>\n",
       "      <td>32.095055</td>\n",
       "      <td>2.782411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>selu</td>\n",
       "      <td>48</td>\n",
       "      <td>0.179944</td>\n",
       "      <td>0.000104</td>\n",
       "      <td>12</td>\n",
       "      <td>128</td>\n",
       "      <td>8</td>\n",
       "      <td>0.754098</td>\n",
       "      <td>34.739235</td>\n",
       "      <td>2.444146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>selu</td>\n",
       "      <td>187</td>\n",
       "      <td>0.370754</td>\n",
       "      <td>0.000257</td>\n",
       "      <td>10</td>\n",
       "      <td>12</td>\n",
       "      <td>9</td>\n",
       "      <td>0.655738</td>\n",
       "      <td>37.315197</td>\n",
       "      <td>2.376034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>selu</td>\n",
       "      <td>153</td>\n",
       "      <td>0.589909</td>\n",
       "      <td>0.001054</td>\n",
       "      <td>11</td>\n",
       "      <td>94</td>\n",
       "      <td>10</td>\n",
       "      <td>0.819672</td>\n",
       "      <td>39.834914</td>\n",
       "      <td>2.320186</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  activation  batch_size  dropout_rate  learning_rate  num_epochs  units  id  \\\n",
       "0     linear          17      0.311632       0.000676          71     97   1   \n",
       "1       gelu          28      0.221418       0.000016          60     68   2   \n",
       "2     linear          72      0.136193       0.000573          70    126   3   \n",
       "3       gelu          98      0.394199       0.005590          65    118   4   \n",
       "4       selu         161      0.240164       0.000459          41    128   5   \n",
       "5        elu         168      0.396514       0.000036          10    128   6   \n",
       "6       selu          34      0.064309       0.000546          13    114   7   \n",
       "7       selu          48      0.179944       0.000104          12    128   8   \n",
       "8       selu         187      0.370754       0.000257          10     12   9   \n",
       "9       selu         153      0.589909       0.001054          11     94  10   \n",
       "\n",
       "   objective  elapsed_sec  duration  \n",
       "0   0.786885     8.032742  7.282537  \n",
       "1   0.721311    13.982644  5.657617  \n",
       "2   0.803279    18.642096  4.459822  \n",
       "3   0.786885    23.278052  4.436977  \n",
       "4   0.868852    26.683581  3.195485  \n",
       "5   0.475410    29.111158  2.224259  \n",
       "6   0.836066    32.095055  2.782411  \n",
       "7   0.754098    34.739235  2.444146  \n",
       "8   0.655738    37.315197  2.376034  \n",
       "9   0.819672    39.834914  2.320186  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f906a04f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The default configuration has an accuracy of 0.803. The best configuration found by DeepHyper has an accuracy 0.869, trained in 3.20 secondes and finished after 26.68 secondes of search.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'activation': 'selu',\n",
       " 'batch_size': 161,\n",
       " 'dropout_rate': 0.2401638933318062,\n",
       " 'learning_rate': 0.0004585594439639,\n",
       " 'num_epochs': 41,\n",
       " 'units': 128,\n",
       " 'id': 5}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i_max = results.objective.argmax()\n",
    "best_config = results.iloc[results.objective.argmax()][:-3].to_dict()\n",
    "\n",
    "print(f\"The default configuration has an accuracy of {objective_default:.3f}. \" \n",
    "      f\"The best configuration found by DeepHyper has an accuracy {results['objective'].iloc[i_max]:.3f}, \" \n",
    "      f\"trained in {results['duration'].iloc[i_max]:.2f} secondes and \"\n",
    "      f\"finished after {results['elapsed_sec'].iloc[i_max]:.2f} secondes of search.\")\n",
    "\n",
    "\n",
    "\n",
    "best_config"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
