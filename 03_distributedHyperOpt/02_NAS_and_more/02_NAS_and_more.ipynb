{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be0a4427",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. [Imports and GPU Detection](#imports-and-gpu-detection)\n",
    "2. [Start Ray](#Start-Ray)\n",
    "3. [A Synthetic Dataset](#a-synthetic-dataset)\n",
    "4. [Scaling the Data](#scaling-the-data)\n",
    "5. [Baseline Neural Network](#baseline-neural-network)\n",
    "6. [Define the Neural Architecture Search Space](#define-the-neural-architecture-search-space)\n",
    "7. [Define the Neural Architecture Search Optimization Problem](#define-the-neural-architecture-search-optimization-problem)\n",
    "8. [Define the Evaluator Object](#Define-the-Evaluator-Object)\n",
    "9. [Define and Run the Neural Architecture Search](#Define-and-Run-the-Neural-Architecture-Search)\n",
    "10. [Adding Uncertainty Quantification to the Baseline Neural Network](#Adding-Uncertainty-Quantification-to-the-Baseline-Neural-Network)\n",
    "11. [Ensemble of Reural Networks With Random Initialization](#Ensemble-of-Reural-Networks-With-Random-Initialization)\n",
    "12. [AutoDEUQ: Automated Deep Ensemble with Uncertainty Quantification](#AutoDEUQ:-Automated-Deep-Ensemble-with-Uncertainty-Quantification)\n",
    "\n",
    "## Imports and GPU Detection <a class=\"anchor\" id=\"imports-and-gpu-detection\"></a>\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "<b>Warning</b>\n",
    "    \n",
    "By design asyncio does not allow nested event loops. Jupyter is using Tornado which already starts an event loop. Therefore the following patch is required to run this tutorial.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4474f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nest_asyncio\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a9ee9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pathlib\n",
    "import shutil\n",
    "\n",
    "!export TF_CPP_MIN_LOG_LEVEL=3\n",
    "!export TF_XLA_FLAGS=--tf_xla_enable_xla_devices\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38664b59",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "<b>Note</b>\n",
    "    \n",
    "The `TF_CPP_MIN_LOG_LEVEL` can be used to avoid the logging of Tensorflow *DEBUG*, *INFO* and *WARNING* statements.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8925ac7f",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "<b>Note</b>\n",
    "    \n",
    "The following can be used to detect if **GPU** devices are available on the current host.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5e0602",
   "metadata": {},
   "outputs": [],
   "source": [
    "available_gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "n_gpus = len(available_gpus)\n",
    "if n_gpus > 1:\n",
    "    n_gpus -= 1\n",
    "is_gpu_available = n_gpus > 0\n",
    "\n",
    "if is_gpu_available:\n",
    "    print(f\"{n_gpus} GPU{'s are' if n_gpus > 1 else ' is'} available.\")\n",
    "else:\n",
    "    print(\"No GPU available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c997c83",
   "metadata": {},
   "source": [
    "## Start Ray\n",
    "\n",
    "We launch the Ray run-time depending on the detected local ressources. If GPU(s) is(are) detected then 1 worker is started for each GPU. If not, then only 1 worker is started. You can start more workers by setting `num_cpus=1` to a value greater than 1.\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "<b>Warning</b>\n",
    "    \n",
    "In the case of GPUs it is important to follow this scheme to avoid multiple processes (Ray workers vs current process) to lock the same GPU.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b522cd81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "\n",
    "if not(ray.is_initialized()):\n",
    "    if is_gpu_available:\n",
    "        ray.init(num_cpus=n_gpus, num_gpus=n_gpus, log_to_driver=False)            \n",
    "    else:\n",
    "        ray.init(num_cpus=1, log_to_driver=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c702c644",
   "metadata": {},
   "source": [
    "## A Synthetic Dataset <a class=\"anchor\" id=\"a-synthetic-dataset\"></a>\n",
    "\n",
    "Now, we will start by defining our artificial dataset based on a Sinus curve. We will first generate data for a **training set** (used for estimation) and a **testing set** (used to evaluate the final performance). Then the training set will be sub-divided in a new **training set** (used to estimate the neural network weights) and **validation set** (used to estimate the neural network hyperparameters and architecture). The data are generated from the following function:\n",
    "\n",
    "$$ y = f(x) = 2 \\cdot \\sin(x) + \\epsilon$$\n",
    "\n",
    "The training data will be generated in a range between $[-30, -20]$ with $\\epsilon \\sim \\mathcal{N}(0,0.25)$ and  in a range between $[20, 30]$ with $\\epsilon \\sim \\mathcal{N}(0,1)$. The code for the training data is then corresponding to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9499cd94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_train_test(random_state=42):\n",
    "    rs = np.random.RandomState(random_state)\n",
    "\n",
    "    train_size = 400\n",
    "    f = lambda x: 2*np.sin(x) # a simlpe affine function\n",
    "\n",
    "    x_1 = rs.uniform(low=-30, high=-20.0, size=train_size//2)\n",
    "    eps_1 = rs.normal(loc=0.0, scale=0.5, size=train_size//2)\n",
    "    y_1 = f(x_1) + eps_1\n",
    "\n",
    "    x_2 = rs.uniform(low=20.0, high=30.0, size=train_size//2)\n",
    "    eps_2 = rs.normal(loc=0.0, scale=1.0, size=train_size//2)\n",
    "    y_2 = f(x_2) + eps_2\n",
    "\n",
    "    x = np.concatenate([x_1, x_2], axis=0)\n",
    "    y = np.concatenate([y_1, y_2], axis=0)\n",
    "\n",
    "    x_tst = np.linspace(-40.0, 40.0, 200)\n",
    "    y_tst = f(x_tst)\n",
    "\n",
    "    x = x.reshape(-1, 1)\n",
    "    y = y.reshape(-1, 1)\n",
    "\n",
    "    x_tst = x_tst.reshape(-1, 1)\n",
    "    y_tst = y_tst.reshape(-1, 1)\n",
    "\n",
    "    return (x, y), (x_tst, y_tst)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92bbf261",
   "metadata": {},
   "source": [
    "Then the code to split the training data in a new **training set** and a **validation set** corresponds to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d780cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def load_data_train_valid(verbose=0, random_state=42):\n",
    "\n",
    "    (x, y), _ = load_data_train_test(random_state=random_state)\n",
    "\n",
    "    train_X, valid_X, train_y, valid_y = train_test_split(\n",
    "        x, y, test_size=0.33, random_state=random_state\n",
    "    )\n",
    "\n",
    "    if verbose:\n",
    "        print(f'train_X shape: {np.shape(train_X)}')\n",
    "        print(f'train_y shape: {np.shape(train_y)}')\n",
    "        print(f'valid_X shape: {np.shape(valid_X)}')\n",
    "        print(f'valid_y shape: {np.shape(valid_y)}')\n",
    "    return (train_X, train_y), (valid_X, valid_y)\n",
    "\n",
    "\n",
    "(x, y), (vx, vy) = load_data_train_valid(verbose=1)\n",
    "_, (tx , ty) = load_data_train_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73734b1c",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "<b>Note</b>\n",
    "    \n",
    "When it is possible to factorize the two previous function into one, DeepHyper interface requires a function which returns `(train_inputs, train_outputs), (valid_inputs, valid_outputs)`.\n",
    "    \n",
    "</div>\n",
    "\n",
    "We can give a visualization of this data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43c6cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "width = 8\n",
    "height = width/1.618\n",
    "plt.figure(figsize=(width, height))\n",
    "\n",
    "plt.plot(tx.reshape(-1), ty.reshape(-1), \"ko--\", label=\"test\", alpha=0.5)\n",
    "plt.plot(x.reshape(-1), y.reshape(-1), \"bo\", label=\"train\", alpha=0.8)\n",
    "plt.plot(vx.reshape(-1), vy.reshape(-1), \"ro\", label=\"valid\", alpha=0.8)\n",
    "\n",
    "plt.ylabel(\"$y = f(x)$\", fontsize=12)\n",
    "plt.xlabel(\"$x$\", fontsize=12)\n",
    "\n",
    "plt.xlim(-40, 40)\n",
    "plt.legend(loc=\"upper center\", ncol=3, fontsize=12)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d12496",
   "metadata": {},
   "source": [
    "## Scaling the Data <a class=\"anchor\" id=\"scaling-the-data\"></a>\n",
    "\n",
    "It is important to apply standard scaling on the input/output data to have faster convergence when training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0566aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "scaler_x = StandardScaler()\n",
    "s_x = scaler_x.fit_transform(x)\n",
    "s_vx = scaler_x.transform(vx)\n",
    "s_tx = scaler_x.transform(tx)\n",
    "\n",
    "scaler_y = StandardScaler()\n",
    "s_y = scaler_y.fit_transform(y)\n",
    "s_vy = scaler_y.transform(vy)\n",
    "s_ty = scaler_y.transform(ty)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ceb5938",
   "metadata": {},
   "source": [
    "## Baseline Neural Network <a class=\"anchor\" id=\"baseline-neural-network\"></a>\n",
    "\n",
    "Let us define a baseline neural network based on a regular multi-layer perceptron architecture which learn the mean estimate and minimise the mean squared error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996814d7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "input_ = tf.keras.layers.Input(shape=(1,))\n",
    "out = tf.keras.layers.Dense(200, activation=\"relu\")(input_)\n",
    "out = tf.keras.layers.Dense(200, activation=\"relu\")(out)\n",
    "output = tf.keras.layers.Dense(1)(out)\n",
    "model = tf.keras.Model(input_, output)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "model.compile(optimizer, \"mse\")\n",
    "\n",
    "history = model.fit(s_x, s_y, epochs=200, batch_size=4, validation_data=(s_vx, s_vy), verbose=1).history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee1d2d0",
   "metadata": {},
   "source": [
    "We can do a vizualisation of our learning curves to make sure the training and validation loss decrease correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993871d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "width = 8\n",
    "height = width/1.618\n",
    "plt.figure(figsize=(width, height))\n",
    "\n",
    "plt.plot(history[\"loss\"], label=\"training\")\n",
    "plt.plot(history[\"val_loss\"], label=\"validation\")\n",
    "\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"NLL\")\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c375374",
   "metadata": {},
   "source": [
    "Also, let us look at the prediction on the test set after reversing the scaling of predicted variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b53fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_s_ty = model(s_tx)\n",
    "pred_ty = scaler_y.inverse_transform(pred_s_ty)\n",
    "\n",
    "width = 8\n",
    "height = width/1.618\n",
    "plt.figure(figsize=(width, height))\n",
    "\n",
    "plt.plot(tx, ty, label=\"truth\")\n",
    "plt.plot(tx, pred_ty, label=\"pred\")\n",
    "\n",
    "y_lim = 10\n",
    "plt.fill_between([-30, -20], [-y_lim, -y_lim], [y_lim, y_lim], color=\"grey\", alpha=0.5)\n",
    "plt.fill_between([20, 30], [-y_lim, -y_lim], [y_lim, y_lim], color=\"grey\", alpha=0.5)\n",
    "\n",
    "plt.legend()\n",
    "plt.ylim(-y_lim, y_lim)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8743319",
   "metadata": {},
   "source": [
    "## Define the Neural Architecture Search Space <a class=\"anchor\" id=\"define-the-neural-architecture-search-space\"></a>\n",
    "\n",
    "The neural architecture search space is composed of discrete decision variables. For each decision variable we choose among a list of possible operation to perform (e.g., fully connected, ReLU). To define this search space, it is necessary to use two classes:\n",
    "\n",
    "* `KSearchSpace` (for Keras Search Space): represents a directed acyclic graph (DAG) in which each node represents a chosen operation. It represents the possible neural networks that can be created.\n",
    "* `SpaceFactory`: is a utilitiy class used to pack the logic of a search space definition and share it with others.\n",
    "\n",
    "Then, inside a `KSearchSpace` we will have two types of nodes:\n",
    "* `VariableNode`: corresponds to discrete decision variables and are used to define a list of possible operation.\n",
    "* `ConstantNode`: corresponds to fixed operation in the search space (e.g., input/outputs)\n",
    "\n",
    "Finally, it is possible to reuse any `tf.keras.layers` to define a `KSearchSpace`. However, it is important to wrap each layer in an `operation` to perform a lazy memory allocation of tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e9b655",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "from deephyper.nas.space import KSearchSpace, SpaceFactory\n",
    "\n",
    "# Decision variables are represented by nodes in a graph\n",
    "from deephyper.nas.space.node import ConstantNode, VariableNode\n",
    "\n",
    "# For Skip/Residual connections\n",
    "from deephyper.nas.space.op.basic import Zero\n",
    "from deephyper.nas.space.op.connect import Connect\n",
    "from deephyper.nas.space.op.merge import AddByProjecting\n",
    "\n",
    "from deephyper.nas.space.op.op1d import Identity\n",
    "\n",
    "# The \"operation\" creates a wrapper around Keras layers\n",
    "# to avoid allocating memory each time a new layer is defined\n",
    "# in the search space\n",
    "from deephyper.nas.space.op import operation\n",
    "\n",
    "Dense = operation(tf.keras.layers.Dense)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42fd4eb3",
   "metadata": {},
   "source": [
    "We implement the `build` method of the `RegressionFactory` which is a subclass of `SpaceFactory`. The `build` method interface is:\n",
    "\n",
    "```python\n",
    "def build(self, input_shape, output_shape, **kwargs) -> KSearchSpace:\n",
    "    ...\n",
    "```\n",
    "\n",
    "where:\n",
    "* `input_shape` corresponds to a tuple or a list of tuple indicating the shapes of inputs tensors.\n",
    "* `output_shape` corresponds to the same but of output_tensors.\n",
    "* `**kwargs` denotes that any other key word argument can be defined by the user.\n",
    "\n",
    "This method has to return an object of type `KSearchSpace`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f99f031",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegressionFactory(SpaceFactory):\n",
    "    \n",
    "    # Possible activation functions\n",
    "    ACTIVATIONS = [\n",
    "        tf.keras.activations.elu,\n",
    "        tf.keras.activations.gelu,\n",
    "        tf.keras.activations.hard_sigmoid,\n",
    "        tf.keras.activations.linear,\n",
    "        tf.keras.activations.relu,\n",
    "        tf.keras.activations.selu,\n",
    "        tf.keras.activations.sigmoid,\n",
    "        tf.keras.activations.softplus,\n",
    "        tf.keras.activations.softsign,\n",
    "        tf.keras.activations.swish,\n",
    "        tf.keras.activations.tanh,\n",
    "    ]\n",
    "    \n",
    "    def build(self, input_shape, output_shape, num_layers=3, **kwargs):\n",
    "\n",
    "        self.ss = KSearchSpace(input_shape, output_shape)\n",
    "        \n",
    "        # After creating a KSearchSpace nodes corresponds to the inputs are directly accessible\n",
    "        out_sub_graph = self.build_sub_graph(self.ss.input_nodes[0], num_layers)\n",
    "\n",
    "        output = ConstantNode(op=Dense(output_shape[0]))  \n",
    "        self.ss.connect(out_sub_graph, output)\n",
    "\n",
    "        return self.ss\n",
    "\n",
    "    def build_sub_graph(self, input_node, num_layers=3):\n",
    "        \n",
    "\n",
    "        # Look over skip connections within a range of the 3 previous nodes\n",
    "        anchor_points = collections.deque([input_node], maxlen=3)\n",
    "        \n",
    "        prev_node = input_node\n",
    "        \n",
    "        for _ in range(num_layers):\n",
    "            \n",
    "            # Create a variable node to list possible \"Dense\" layers\n",
    "            dense = VariableNode()\n",
    "            \n",
    "            # Add the possible operations to the dense node\n",
    "            self.add_dense_to_(dense)\n",
    "            \n",
    "            # Connect the previous node to the dense node\n",
    "            self.ss.connect(prev_node, dense)\n",
    "\n",
    "            # Create a constant node to merge all input connections\n",
    "            merge = ConstantNode()\n",
    "            merge.set_op(\n",
    "                AddByProjecting(self.ss, [dense], activation=\"relu\")\n",
    "            )\n",
    "\n",
    "            for node in anchor_points:\n",
    "                \n",
    "                # Create a variable node for each possible connection\n",
    "                skipco = VariableNode()\n",
    "                \n",
    "                skipco.add_op(Zero()) # corresponds to no connection\n",
    "                skipco.add_op(Connect(self.ss, node)) # corresponds to (node => skipco)\n",
    "                \n",
    "                # Connect the (skipco => merge)\n",
    "                self.ss.connect(skipco, merge)\n",
    "\n",
    " \n",
    "            # ! for next iter\n",
    "            prev_node = merge\n",
    "            anchor_points.append(prev_node)\n",
    "\n",
    "        return prev_node\n",
    "\n",
    "    def add_dense_to_(self, node):\n",
    "        \n",
    "        # We add the \"Identity\" operation to allow the choice \"doing nothing\"\n",
    "        node.add_op(Identity())\n",
    "        \n",
    "        step = 16\n",
    "        for units in range(step, step * 16 + 1, step):\n",
    "            for activation in self.ACTIVATIONS:\n",
    "                node.add_op(Dense(units=units, activation=activation))\n",
    "\n",
    "\n",
    "def create_search_space(input_shape=(1,), output_shape=(1,), **kwargs):    \n",
    "    return RegressionFactory()(input_shape, output_shape, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26483b5a",
   "metadata": {},
   "source": [
    "Let us visualize a few randomly sampled neural architecture from this search space.\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "<b>Warning</b>\n",
    "    \n",
    "To execute the following it is necessary to have `pydot` and `graphviz` installed. A possibility is to have a conda environment and do:\n",
    "    \n",
    "```console\n",
    "pip install pydot\n",
    "conda install graphviz\n",
    "```\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b57103",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.image as mpimg\n",
    "\n",
    "\n",
    "shapes = dict(input_shape=(1,), output_shape=(1,))\n",
    "factory = RegressionFactory()\n",
    "    \n",
    "\n",
    "images = []\n",
    "plt.figure(figsize=(15,15))\n",
    "for i in range(4):\n",
    "\n",
    "    plt.subplot(2,2,i+1)\n",
    "    factory.plot_model(**shapes, show_shapes=False, show_layer_names=False)\n",
    "    image = mpimg.imread(\"random_model.png\")\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265a3a44",
   "metadata": {},
   "source": [
    "## Define the Neural Architecture Optimization Problem <a class=\"anchor\" id=\"define-the-neural-architecture-search-optimization-problem\"></a>\n",
    "\n",
    "In order to define a neural architecture search problem we have to use the `NaProblem` class. This class gives access to different method for the user to customize the training settings of neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea447c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deephyper.problem import NaProblem\n",
    "\n",
    "\n",
    "def stdscaler():\n",
    "    return StandardScaler()\n",
    "\n",
    "\n",
    "problem = NaProblem()\n",
    "\n",
    "# Bind a function which returns (train_input, train_output), (valid_input, valid_output)\n",
    "problem.load_data(load_data_train_valid)\n",
    "\n",
    "# Bind a function which return a scikit-learn preprocessor (with fit, fit_transform, inv_transform...etc)\n",
    "problem.preprocessing(stdscaler)\n",
    "\n",
    "# Bind a function which returns a search space and give some arguments for the `build` method\n",
    "problem.search_space(create_search_space, num_layers=3)\n",
    "\n",
    "# Define a set of fixed hyperparameters for all trained neural networks\n",
    "problem.hyperparameters(\n",
    "    batch_size=4,\n",
    "    learning_rate=1e-3,\n",
    "    optimizer=\"adam\",\n",
    "    num_epochs=200,\n",
    "    callbacks=dict(\n",
    "        EarlyStopping=dict(monitor=\"val_loss\", mode=\"min\", verbose=0, patience=30)\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Define the loss to minimize\n",
    "problem.loss(\"mse\")\n",
    "\n",
    "# Define complementary metrics\n",
    "problem.metrics([])\n",
    "\n",
    "# Define the maximized objective. Here we take the negative of the validation loss.\n",
    "problem.objective(\"-val_loss\")\n",
    "\n",
    "problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea800ab1",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "<b>Tip</b>\n",
    "    \n",
    "Adding an `EarlyStopping(...)` callback is a good idea to stop the training of your model as soon as it stops to improve.\n",
    "\n",
    "```python\n",
    "...\n",
    "EarlyStopping=dict(monitor=\"val_loss\", mode=\"min\", verbose=0, patience=30)\n",
    "...\n",
    "```\n",
    "    \n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4161c7b",
   "metadata": {},
   "source": [
    "## Define the Evaluator Object\n",
    "\n",
    "The `Evaluator` object is responsible of defining the backend used to distribute the function evaluation in DeepHyper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e798a7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deephyper.evaluator.evaluate import Evaluator\n",
    "from deephyper.evaluator.callback import LoggerCallback\n",
    "\n",
    "\n",
    "def get_evaluator(run_function):\n",
    "    \n",
    "    # Default arguments for Ray: 1 worker and 1 worker per evaluation\n",
    "    method_kwargs = {\n",
    "        \"num_cpus\": 1, \n",
    "        \"num_cpus_per_task\": 1,\n",
    "        \"callbacks\": [LoggerCallback()] # To interactively follow the finished evaluations,\n",
    "    }\n",
    "\n",
    "    # If GPU devices are detected then it will create 'n_gpus' workers\n",
    "    # and use 1 worker for each evaluation\n",
    "    if is_gpu_available:\n",
    "        method_kwargs[\"num_cpus\"] = n_gpus\n",
    "        method_kwargs[\"num_gpus\"] = n_gpus\n",
    "        method_kwargs[\"num_cpus_per_task\"] = 1\n",
    "        method_kwargs[\"num_gpus_per_task\"] = 1\n",
    "\n",
    "    evaluator = Evaluator.create(\n",
    "        run_function, \n",
    "        method=\"ray\", \n",
    "        method_kwargs=method_kwargs\n",
    "    )\n",
    "    print(f\"Created new evaluator with {evaluator.num_workers} worker{'s' if evaluator.num_workers > 1 else ''} and config: {method_kwargs}\", )\n",
    "    \n",
    "    return evaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e31cdd",
   "metadata": {},
   "source": [
    "For neural architecture search a standard training pipeline is provided by the `deephyper.nas.run.alpha.run` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db3be52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deephyper.nas.run.alpha import run as run_nas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845ab242",
   "metadata": {},
   "source": [
    "## Define and Run the Neural Architecture Search\n",
    "\n",
    "All search algorithms follow a similar interface. A `problem` and `evaluator` object has to be provided to the search then the search can be executed through the `search(max_evals, timeout)` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11781742",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {} # used to store the results of different search algorithms\n",
    "max_evals = 250 # maximum number of iteratins for all searches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc5ff9c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from deephyper.search.nas.random import Random\n",
    "\n",
    "random_search = Random(problem, get_evaluator(run_nas))\n",
    "\n",
    "results[\"random\"] = random_search.search(max_evals=max_evals)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662a0661",
   "metadata": {},
   "source": [
    "By default, the `RegularizedEvolution` has a population size of 100 therefore, it will start optimizing only after 100 evaluations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09284489",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from deephyper.search.nas.regevo import RegularizedEvolution\n",
    "\n",
    "regevo_search = RegularizedEvolution(problem, get_evaluator(run_nas))\n",
    "\n",
    "results[\"regevo\"] = regevo_search.search(max_evals=max_evals)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8012d8f9",
   "metadata": {},
   "source": [
    "We can now compare the search trajectories for different algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54334763",
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_score(l):\n",
    "    r = [l[0]]\n",
    "    for el in l[1:]:\n",
    "        r.append(max(r[-1], el))\n",
    "    return r\n",
    "\n",
    "width = 8\n",
    "height = width/1.618\n",
    "plt.figure(figsize=(width, height))\n",
    "\n",
    "for k, result in results.items():\n",
    "    plt.plot(max_score(results[k].objective), label=k)\n",
    "    \n",
    "plt.legend()\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df8f3d31",
   "metadata": {},
   "source": [
    "If we look at the dataframe of results for each search we will find it slightly different than the one of hyperparameter search. A new column `arch_seq` corresponds to an embedding for each evaluated architecture. Each integer of an `arch_seq` list corresponds to the choice of a `VariableNode` in our `KSearchSpace`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c52c9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "results[\"random\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0bc94ea",
   "metadata": {},
   "source": [
    "Let us visualize the best architecture found:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c32a210",
   "metadata": {},
   "outputs": [],
   "source": [
    "i_max = results[\"random\"].objective.argmax()\n",
    "best_score = results[\"random\"].iloc[i_max].objective\n",
    "best_arch_seq = json.loads(results[\"random\"].iloc[i_max].arch_seq)\n",
    "\n",
    "factory.plot_model(arch_seq=best_arch_seq, **shapes, show_shapes=True, show_layer_names=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac4693c5",
   "metadata": {},
   "source": [
    "## Adding Uncertainty Quantification to the Baseline Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162095e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_probability as tfp\n",
    "tfd = tfp.distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0624c024",
   "metadata": {},
   "source": [
    "To perform uncertainty quantitifcation, instead of minimising the mean squared error we will minimize the negative log-likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2486879d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nll(y, rv_y):\n",
    "    \"\"\"Negative log likelihood for Tensorflow probability.\"\"\"\n",
    "    return -rv_y.log_prob(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dfd2d80",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "input_ = tf.keras.layers.Input(shape=(1,))\n",
    "out = tf.keras.layers.Dense(200, activation=\"relu\")(input_)\n",
    "out = tf.keras.layers.Dense(200, activation=\"relu\")(out)\n",
    "\n",
    "# For each predicted variable (1) we need the mean and variance estimate\n",
    "out = tf.keras.layers.Dense(1*2)(out) \n",
    "\n",
    "# We feed these estimates to output a Normal distribution for each predicted variable\n",
    "output = tfp.layers.DistributionLambda(\n",
    "            lambda t: tfd.Normal(\n",
    "                loc=t[..., :1],\n",
    "                scale=1e-3 + tf.math.softplus(0.05 * t[..., 1:]), # positive constraint on the standard dev.\n",
    "            )\n",
    "        )(out)\n",
    "\n",
    "model_uq = tf.keras.Model(input_, output)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "model_uq.compile(optimizer, loss=nll)\n",
    "\n",
    "history = model_uq.fit(s_x, s_y, epochs=200, batch_size=4, validation_data=(s_vx, s_vy), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f2e26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "width = 8\n",
    "height = width/1.618\n",
    "plt.figure(figsize=(width, height))\n",
    "\n",
    "plt.plot(history.history[\"loss\"], label=\"training\")\n",
    "plt.plot(history.history[\"val_loss\"], label=\"validation\")\n",
    "\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"NLL\")\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e26b978",
   "metadata": {},
   "source": [
    "Let us visualize the learned uncertainty estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a6f947",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_s_ty = model_uq(s_tx)\n",
    "\n",
    "pred_ty_mean = pred_s_ty.loc.numpy() + scaler_y.mean_\n",
    "pred_ty_var = np.square(pred_s_ty.scale.numpy()) * scaler_y.var_\n",
    "\n",
    "width = 8\n",
    "height = width/1.618\n",
    "plt.figure(figsize=(width, height))\n",
    "\n",
    "plt.plot(tx, ty, label=\"truth\")\n",
    "plt.plot(tx, pred_ty_mean, label=\"$\\mu$\")\n",
    "plt.fill_between(\n",
    "    tx.reshape(-1), \n",
    "    (pred_ty_mean - pred_ty_var).reshape(-1), \n",
    "    (pred_ty_mean + pred_ty_var).reshape(-1), \n",
    "    color=\"orange\", \n",
    "    alpha=0.5,\n",
    "    label=\"$\\sigma^2$\"\n",
    ")\n",
    "\n",
    "y_lim = 10\n",
    "plt.fill_between([-30, -20], [-y_lim, -y_lim], [y_lim, y_lim], color=\"grey\", alpha=0.5)\n",
    "plt.fill_between([20, 30], [-y_lim, -y_lim], [y_lim, y_lim], color=\"grey\", alpha=0.5)\n",
    "\n",
    "plt.legend()\n",
    "plt.ylim(-y_lim, y_lim)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7d4cca",
   "metadata": {},
   "source": [
    "The learned mean estimates appears to be worse than when minimizing the mean squared error loss. Also, we can see than the variance estimate are not meaningful in areas missing data (white background) and do not learn properly the noise in are with data (grey background)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5b952b",
   "metadata": {},
   "source": [
    "## Ensemble of Reural Networks With Random Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d3fa3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_model(model_id):\n",
    "    \n",
    "    # Model\n",
    "    input_ = tf.keras.layers.Input(shape=(1,))\n",
    "    out = tf.keras.layers.Dense(200, activation=\"relu\")(input_)\n",
    "    out = tf.keras.layers.Dense(200, activation=\"relu\")(out)\n",
    "    out = tf.keras.layers.Dense(2)(out) # 1 unit for the mean, 1 unit for the scale\n",
    "    output = tfp.layers.DistributionLambda(\n",
    "                lambda t: tfd.Normal(\n",
    "                    loc=t[..., :1],\n",
    "                    scale=1e-3 + tf.math.softplus(0.05 * t[..., 1:]),\n",
    "                )\n",
    "            )(out)\n",
    "    model_uq = tf.keras.Model(input_, output)\n",
    "    \n",
    "    \n",
    "    model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=os.path.join(\"models_random_init\", f\"{model_id}.h5\"), \n",
    "        monitor='val_loss', \n",
    "        verbose=0, \n",
    "        save_best_only=True,\n",
    "        save_weights_only=False, \n",
    "        mode='min', \n",
    "        save_freq='epoch'\n",
    "    )\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "    model_uq.compile(optimizer, loss=nll)\n",
    "\n",
    "    history = model_uq.fit(s_x, s_y, \n",
    "                           epochs=200, \n",
    "                           batch_size=4, \n",
    "                           validation_data=(s_vx, s_vy), \n",
    "                           verbose=0,\n",
    "                           callbacks=[model_checkpoint_callback]\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e591789",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(\"models_random_init\"):\n",
    "    shutil.rmtree(\"models_random_init\")\n",
    "pathlib.Path(\"models_random_init\").mkdir(parents=False, exist_ok=False)\n",
    "\n",
    "for model_id in tqdm(range(10)):\n",
    "    generate_model(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1527858",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deephyper.ensemble import UQBaggingEnsembleRegressor\n",
    "\n",
    "ensemble = UQBaggingEnsembleRegressor(\n",
    "    model_dir=\"models_random_init\",\n",
    "    loss=nll,  # default is nll\n",
    "    size=5,\n",
    "    verbose=True,\n",
    "    ray_address=\"auto\",\n",
    "    num_cpus=1,\n",
    "    num_gpus=1 if is_gpu_available else None,\n",
    "    selection=\"topk\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3675c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble.fit(s_vx, s_vy)\n",
    "\n",
    "print(f\"Selected members are: \", ensemble.members_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b5495f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_s_ty, pred_s_ty_aleatoric_var, pred_s_ty_epistemic_var = ensemble.predict_var_decomposition(s_tx)\n",
    "\n",
    "pred_ty_mean = pred_s_ty.loc.numpy() + scaler_y.mean_\n",
    "pred_ty_aleatoric_var = pred_s_ty_aleatoric_var * scaler_y.var_\n",
    "pred_ty_epistemic_var = pred_s_ty_epistemic_var * scaler_y.var_\n",
    "\n",
    "width = 8\n",
    "height = width/1.618\n",
    "plt.figure(figsize=(width, height))\n",
    "\n",
    "plt.plot(tx, ty, label=\"truth\")\n",
    "plt.plot(tx, pred_ty_mean, label=\"$\\mu$\")\n",
    "plt.fill_between(\n",
    "    tx.reshape(-1), \n",
    "    (pred_ty_mean - pred_ty_aleatoric_var).reshape(-1), \n",
    "    (pred_ty_mean + pred_ty_aleatoric_var).reshape(-1), \n",
    "    color=\"yellow\", \n",
    "    alpha=0.5,\n",
    "    label=\"$E[\\sigma^2]$: aleatoric\"\n",
    ")\n",
    "plt.fill_between(\n",
    "    tx.reshape(-1), \n",
    "    (pred_ty_mean - pred_ty_aleatoric_var).reshape(-1), \n",
    "    (pred_ty_mean - pred_ty_aleatoric_var - pred_s_ty_epistemic_var).reshape(-1), \n",
    "    color=\"orange\", \n",
    "    alpha=0.5,\n",
    "    label=\"$V[\\mu]$: epistemic\"\n",
    ")\n",
    "plt.fill_between(\n",
    "    tx.reshape(-1), \n",
    "    (pred_ty_mean + pred_ty_aleatoric_var).reshape(-1), \n",
    "    (pred_ty_mean + pred_ty_aleatoric_var + pred_s_ty_epistemic_var).reshape(-1), \n",
    "    color=\"orange\", \n",
    "    alpha=0.5,\n",
    "#     label=\"$V[\\mu]$: epistemic\"\n",
    ")\n",
    "\n",
    "y_lim = 10\n",
    "plt.fill_between([-30, -20], [-y_lim, -y_lim], [y_lim, y_lim], color=\"grey\", alpha=0.5)\n",
    "plt.fill_between([20, 30], [-y_lim, -y_lim], [y_lim, y_lim], color=\"grey\", alpha=0.5)\n",
    "\n",
    "plt.legend()\n",
    "plt.ylim(-y_lim, y_lim)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac13f4c",
   "metadata": {},
   "source": [
    "## AutoDEUQ: Automated Deep Ensemble with Uncertainty Quantification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245d9451",
   "metadata": {},
   "outputs": [],
   "source": [
    "DistributionLambda = operation(tfp.layers.DistributionLambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2c8007",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegressionUQFactory(SpaceFactory):\n",
    "    \n",
    "    def build(self, input_shape, output_shape, num_layers=3, **kwargs):\n",
    "\n",
    "        self.ss = KSearchSpace(input_shape, output_shape)\n",
    "\n",
    "        out_sub_graph = self.build_sub_graph(self.ss.input_nodes[0], num_layers)\n",
    "        \n",
    "        output_dim = output_shape[0]\n",
    "        output_dense = ConstantNode(op=Dense(output_dim*2))  \n",
    "        self.ss.connect(out_sub_graph, output_dense)\n",
    "        \n",
    "        \n",
    "        output_dist = ConstantNode(\n",
    "            op=DistributionLambda(\n",
    "                lambda t: tfd.Normal(\n",
    "                    loc=t[..., :output_dim],\n",
    "                    scale=1e-3 + tf.math.softplus(0.05 * t[..., output_dim:]),\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "        self.ss.connect(output_dense, output_dist)\n",
    "\n",
    "        return self.ss\n",
    "\n",
    "    def build_sub_graph(self, input_node, num_layers=3):\n",
    "        \n",
    "\n",
    "        # Look over skip connections within a range of the 3 previous nodes\n",
    "        anchor_points = collections.deque([input_node], maxlen=3)\n",
    "        \n",
    "        prev_node = input_node\n",
    "        \n",
    "        for _ in range(num_layers):\n",
    "            \n",
    "            # Create a variable node to list possible \"Dense\" layers\n",
    "            dense = VariableNode()\n",
    "            \n",
    "            # Add the possible operations to the dense node\n",
    "            self.add_dense_to_(dense)\n",
    "            \n",
    "            # Connect the previous node to the dense node\n",
    "            self.ss.connect(prev_node, dense)\n",
    "\n",
    "            # Create a constant node to merge all input connections\n",
    "            merge = ConstantNode()\n",
    "            merge.set_op(\n",
    "                AddByProjecting(self.ss, [dense], activation=\"relu\")\n",
    "            )\n",
    "\n",
    "            for node in anchor_points:\n",
    "                \n",
    "                # Create a variable node for each possible connection\n",
    "                skipco = VariableNode()\n",
    "                \n",
    "                skipco.add_op(Zero()) # corresponds to no connection\n",
    "                skipco.add_op(Connect(self.ss, node)) # corresponds to (node => skipco)\n",
    "                \n",
    "                # Connect the (skipco => merge)\n",
    "                self.ss.connect(skipco, merge)\n",
    "\n",
    " \n",
    "            # ! for next iter\n",
    "            prev_node = merge\n",
    "            anchor_points.append(prev_node)\n",
    "\n",
    "        return prev_node\n",
    "\n",
    "    def add_dense_to_(self, node):\n",
    "        \n",
    "        # We add the \"Identity\" operation to allow the choice \"doing nothing\"\n",
    "        node.add_op(Identity())\n",
    "        \n",
    "        step = 16\n",
    "        for units in range(step, step * 16 + 1, step):\n",
    "            for activation in ACTIVATIONS:\n",
    "                node.add_op(Dense(units=units, activation=activation))\n",
    "\n",
    "\n",
    "def create_search_space_uq(input_shape=(1,), output_shape=(1,), **kwargs):\n",
    "    return RegressionUQFactory()(input_shape, output_shape, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e58a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "problem_uq = NaProblem()\n",
    "\n",
    "problem_uq.load_data(load_data_train_valid)\n",
    "\n",
    "problem_uq.preprocessing(stdscaler)\n",
    "\n",
    "problem_uq.search_space(create_search_space_uq, num_layers=3)\n",
    "\n",
    "problem_uq.hyperparameters(\n",
    "    batch_size=problem_uq.add_hyperparameter((1, 32), \"batch_size\"),\n",
    "    learning_rate=problem_uq.add_hyperparameter(\n",
    "        (1e-4, 0.1, \"log-uniform\"),\n",
    "        \"learning_rate\",\n",
    "    ),\n",
    "    optimizer=problem_uq.add_hyperparameter(\n",
    "        [\"sgd\", \"rmsprop\", \"adagrad\", \"adam\", \"adadelta\", \"adamax\", \"nadam\"],\n",
    "        \"optimizer\",\n",
    "    ),\n",
    "    patience_ReduceLROnPlateau=problem_uq.add_hyperparameter(\n",
    "        (10, 20), \"patience_ReduceLROnPlateau\"\n",
    "    ),\n",
    "    patience_EarlyStopping=problem_uq.add_hyperparameter(\n",
    "        (20, 30), \"patience_EarlyStopping\"\n",
    "    ),\n",
    "    num_epochs=200,\n",
    "    callbacks=dict(\n",
    "        ReduceLROnPlateau=dict(monitor=\"val_loss\", mode=\"min\", verbose=0, patience=5),\n",
    "        EarlyStopping=dict(\n",
    "            monitor=\"val_loss\", mode=\"min\", verbose=0, patience=10\n",
    "        ),\n",
    "        ModelCheckpoint=dict(\n",
    "            monitor=\"val_loss\",\n",
    "            mode=\"min\",\n",
    "            save_best_only=True,\n",
    "            verbose=0,\n",
    "            filepath=\"model.h5\",\n",
    "            save_weights_only=False,\n",
    "        ),\n",
    "    ),\n",
    ")\n",
    "\n",
    "problem_uq.loss(nll)\n",
    "\n",
    "problem_uq.metrics([])\n",
    "\n",
    "# The objective is maximized so we take the negative of the validation loss\n",
    "# where the loss is minimized\n",
    "problem_uq.objective(\"-val_loss\")\n",
    "\n",
    "problem_uq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76643151",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_uq = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ecaccbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deephyper.search.nas.agebo import AgEBO\n",
    "\n",
    "\n",
    "if os.path.exists(\"save\"):\n",
    "    shutil.rmtree(\"save\")\n",
    "\n",
    "agebo_search = AgEBO(problem_uq, get_evaluator(run_nas))\n",
    "\n",
    "results_uq[\"agebo\"] = agebo_search.search(max_evals=max_evals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed55adc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_uq[\"agebo\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae45be22",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble = UQBaggingEnsembleRegressor(\n",
    "    model_dir=\"save/model\",\n",
    "    loss=nll,  # default is nll\n",
    "    size=5,\n",
    "    verbose=True,\n",
    "    ray_address=\"auto\",\n",
    "    num_cpus=1,\n",
    "    num_gpus=1 if is_gpu_available else None,\n",
    "    selection=\"topk\",\n",
    ")\n",
    "\n",
    "ensemble.fit(s_vx, s_vy)\n",
    "\n",
    "print(f\"Selected members are: \", ensemble.members_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1aac98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_s_ty, pred_s_ty_aleatoric_var, pred_s_ty_epistemic_var = ensemble.predict_var_decomposition(s_tx)\n",
    "\n",
    "pred_ty_mean = pred_s_ty.loc.numpy() + scaler_y.mean_\n",
    "pred_ty_aleatoric_var = pred_s_ty_aleatoric_var * scaler_y.var_\n",
    "pred_ty_epistemic_var = pred_s_ty_epistemic_var * scaler_y.var_\n",
    "\n",
    "width = 8\n",
    "height = width/1.618\n",
    "plt.figure(figsize=(width, height))\n",
    "\n",
    "plt.plot(tx, ty, label=\"truth\")\n",
    "plt.plot(tx, pred_ty_mean, label=\"$\\mu$\")\n",
    "plt.fill_between(\n",
    "    tx.reshape(-1), \n",
    "    (pred_ty_mean - pred_ty_aleatoric_var).reshape(-1), \n",
    "    (pred_ty_mean + pred_ty_aleatoric_var).reshape(-1), \n",
    "    color=\"yellow\", \n",
    "    alpha=0.5,\n",
    "    label=\"$E[\\sigma^2]$: aleatoric\"\n",
    ")\n",
    "plt.fill_between(\n",
    "    tx.reshape(-1), \n",
    "    (pred_ty_mean - pred_ty_aleatoric_var).reshape(-1), \n",
    "    (pred_ty_mean - pred_ty_aleatoric_var - pred_s_ty_epistemic_var).reshape(-1), \n",
    "    color=\"orange\", \n",
    "    alpha=0.5,\n",
    "    label=\"$V[\\mu]$: epistemic\"\n",
    ")\n",
    "plt.fill_between(\n",
    "    tx.reshape(-1), \n",
    "    (pred_ty_mean + pred_ty_aleatoric_var).reshape(-1), \n",
    "    (pred_ty_mean + pred_ty_aleatoric_var + pred_s_ty_epistemic_var).reshape(-1), \n",
    "    color=\"orange\", \n",
    "    alpha=0.5,\n",
    ")\n",
    "\n",
    "y_lim = 10\n",
    "plt.fill_between([-30, -20], [-y_lim, -y_lim], [y_lim, y_lim], color=\"grey\", alpha=0.5)\n",
    "plt.fill_between([20, 30], [-y_lim, -y_lim], [y_lim, y_lim], color=\"grey\", alpha=0.5)\n",
    "\n",
    "plt.legend()\n",
    "plt.ylim(-y_lim, y_lim)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be0b659",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
